import os
import requests
from celery_app import app
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_classic.chains import create_retrieval_chain 
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç–∏ –≤–æ—Ä–∫–µ—Ä–∞
rag_chain = None

def init_rag():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG (–∑–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –±–∞–∑—ã)"""
    global rag_chain
    print("Worker: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    
    DB_PATH = "/app/chroma_db_local" # –ü—É—Ç—å –≤–Ω—É—Ç—Ä–∏ Docker
    
    # 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ (CPU)
    embedding_function = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # 2. –ë–∞–∑–∞
    vectorstore = Chroma(
        persist_directory=DB_PATH, 
        embedding_function=embedding_function
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    
    # 3. LLM (Ollama)
    llm = ChatOllama(
        base_url=os.getenv("OLLAMA_BASE_URL"),
        model=os.getenv("MODEL_NAME", "llama3"),
        temperature=0
    )
    
    # 4. –ü—Ä–æ–º–ø—Ç
    system_prompt = (
        "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ ROS 2. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
        "–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å, —Å–∫–∞–∂–∏ '–ù–µ –∑–Ω–∞—é'.\n\n"
        "{context}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])
    
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    print("Worker: RAG –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")

@app.task(name="process_ros2_query", bind=True)
def process_ros2_query(self, chat_id, user_query):
    """–ó–∞–¥–∞—á–∞ Celery"""
    global rag_chain
    if rag_chain is None:
        init_rag()
        
    try:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = rag_chain.invoke({"input": user_query})
        answer = response["answer"]
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources = set([doc.metadata.get('source', 'unknown').split('/')[-1] for doc in response['context']])
        source_text = "\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join([f"- {s}" for s in sources])
        
        final_text = answer + source_text
        
    except Exception as e:
        final_text = f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

    # –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ Telegram (–Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ API, –º–∏–Ω—É—è –±–æ—Ç–∞-–ø—Ä–∏–µ–º—â–∏–∫–∞)
    token = os.getenv("TELEGRAM_BOT_TOKEN")
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    data = {
        "chat_id": chat_id,
        "text": final_text,
        "parse_mode": "Markdown"
    }
    requests.post(url, json=data)
    
    return "OK"